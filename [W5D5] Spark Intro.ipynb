{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Intro\n",
    "\n",
    "**Important**: You have to run this notebook using `pyspark` command.\n",
    "\n",
    "1. Create the `SparkSession`\n",
    "2. Open spark dashboard in  [\"http://localhost:4040\"](\"http://localhost:4040\") \n",
    "\n",
    "https://spark.apache.org/docs/latest/monitoring.html\n",
    "\n",
    "Every SparkContext launches a web UI, by default on port 4040, that displays useful information about the application. This includes:\n",
    "\n",
    "A list of scheduler stages and tasks\n",
    "A summary of RDD sizes and memory usage\n",
    "Environmental information.\n",
    "Information about the running executors\n",
    "You can access this interface by simply opening http://<driver-node>:4040 in a web browser. If multiple SparkContexts are running on the same host, they will bind to successive ports beginning with 4040 (4041, 4042, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a SparkContext\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Spark Dashboard in browser tab\n",
    "import webbrowser\n",
    "webbrowser.open(\"http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "NUM_SAMPLES = 10000\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside).count()\n",
    "print(\"Pi is roughly {}\".format(4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Spark commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./data/breadbasket_dms.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping operation with distinct count\n",
    "from pyspark.sql.functions import collect_list, approx_count_distinct\n",
    "q = df.groupby(df.Transaction).agg(collect_list(\"Item\"), approx_count_distinct(\"Item\"))\n",
    "print(q.show())\n",
    "print(q.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_pivoted = df.groupBy(\"Transaction\").pivot(\"Item\").agg(F.lit(1)).na.fill(0)\n",
    "df_pivoted.select(df_pivoted.columns[:7]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_pivoted = df_pivoted.withColumn(\"Transaction\", df_pivoted[\"Transaction\"].cast(IntegerType()))\n",
    "print(df_pivoted.columns[:])\n",
    "print(len(df_pivoted.columns))\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=df_pivoted.columns[3:], outputCol=\"Features\")\n",
    "new_df = vecAssembler.transform(df_pivoted)\n",
    "X = new_df.select('Features')\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "num_clusters = 4\n",
    "clusters = KMeans.train(X.rdd.map(lambda x: x[0].toArray()), num_clusters, maxIterations=15, initializationMode=\"random\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Transaction': df_pivoted.select(\"Transaction\").toPandas()['Transaction'],\n",
    "    'Label': clusters.predict(X.rdd.map(lambda x: x[0].toArray())).collect()\n",
    "})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
